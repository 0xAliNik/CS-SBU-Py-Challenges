{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression is a supervised learning algorithm which is both a statistical and a machine learning algorithm. It is used to predict the real-valued output y based on the given input value x.\n",
    "\n",
    "It depicts the relationship between the dependent variable y and the independent variables xi  ( or features ).  The hypothetical function used for prediction is represented by h( x ).\n",
    "\n",
    "```\n",
    "  h( x ) = w * x + b  \n",
    "  \n",
    "  here, b is the bias.\n",
    "  x represents the feature vector\n",
    "  w represents the weight vector.\n",
    "```\n",
    "\n",
    "Linear regression with one variable is also called univariant linear regression.  After initializing the weight vector, we can find the weight vector to best fit the model by ordinary least squares method or gradient descent learning.\n",
    "\n",
    "### Loss Function\n",
    "The cost function (or loss function) is used to measure the performance of a machine learning model or quantifies the error between the expected values and the values predicted by our hypothetical function. \n",
    "The cost function for Linear Regression is represented by J.\n",
    "\n",
    "$$\n",
    "\\frac{1}{m} \\sum_{i=1}^{m}(y^{(i)}-h(x^{(i)}))^2\n",
    "$$\n",
    "```\n",
    "Here, m is the total number of training examples in the dataset.\n",
    "y(i) represents the value of target variable for ith training example.\n",
    "```\n",
    "\n",
    "So, our objective is to minimize the cost function J (or improve the performance of our machine learning model). To do this, we have to find the weights at which J is minimum.  One such algorithm which can be used to minimize any differentiable function is Gradient Descent. It is a first-order iterative optimizing algorithm that takes us to a minimum of a function.\n",
    "\n",
    "### Gradient descent\n",
    "**Pseudo Code:**\n",
    "\n",
    "1. Start with some w\n",
    "2. Keep changing w to reduce J( w ) until we hopefully end up at a minimum.\n",
    "\n",
    "**Algorithm:** \n",
    "\n",
    "```\n",
    "repeat until convergence  {\n",
    "       tmpi = wi - alpha * dwi          \n",
    "       wi = tmpi              \n",
    "}\n",
    "where alpha is the learning rate.\n",
    "```\n",
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 1)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./dataset/salary_data.csv\")\n",
    "X = df.iloc[:,:-1].values\n",
    "Y = df.iloc[:,1].values\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test= train_test_split(X, Y, test_size=1/3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "\n",
    "    def __init__(self, iterations, learning_rate):\n",
    "        self.iterations = iterations\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "\n",
    "        # m = training no. , n = features no.\n",
    "        self.m , self.n = X.shape\n",
    "\n",
    "        # Parameters Initialization\n",
    "        self.W = np.zeros(self.n)\n",
    "        \n",
    "        self.b = 0\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "        for i in range(self.iterations):\n",
    "            self.update_weights()\n",
    "\n",
    "    def update_weights(self):\n",
    "        #  Gradient Descent\n",
    "\n",
    "        #  Calculate \n",
    "        # dW = \n",
    "        \n",
    "        #  Update W\n",
    "        self.W = self.W - self.learning_rate * dW\n",
    "\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def predict(self):\n",
    "        return self.X.dot(self.W) + self.b\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "    - dW\n",
    "    - dynamic learning_rate\n",
    "\n",
    "    - Multi-feature \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e5ef88d78e85d948188bdbb56d2f103b885cd71ccce37fd482482e8762f43f20"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
